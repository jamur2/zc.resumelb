===============================
Resume-based load balancer pool
===============================

The heart of the resume-based load balancer is the pool, which
implements the load balancing algorithm.  The pool has a collection of
workers organized according to their resumes.

The load balancer works by connecting to workers, creating local
workers for each connection, adding local workers to the pool, and
by accepting wsgi request, getting local workers from the pool and
passing the wsgi requests to the local workers, which, in term,
forwward the requests to the remote workers.

We'll test the pool with stand-ins for the local workers.

    >>> import zc.resumelb.lb
    >>> pool = zc.resumelb.lb.Pool()

The get method is used to get a worker from the pool.  A request class
and an optional timeout is passed. (The timeout is mainly useful for
testing.)

    >>> pool.get('foo', 0.0)

We didn't get a worker (we timed out), because we haven't added one.

    >>> class Worker:
    ...     def __init__(self, name):
    ...         self.name = name
    ...     def __repr__(self):
    ...         return self.name
    ...     def __cmp__(self, other):
    ...         return cmp(self.name, other.name)
    ...     def __hash__(self):
    ...         return hash(self.name)
    ...     def handle(self, *args):
    ...         pass

    >>> w1 = Worker('w1')

    >>> pool.new_resume(w1, {})

   >>> pool.get('foo', 0.0)
   w1

This time, we got the one we registered.

If we create another and register it, we'll still get the original:

   >>> w2 = Worker('w2')
   >>> pool.new_resume(w2, {})

   >>> pool.get('foo')
   w1

 This is because w1 is known to be good at handling foo requests.

 We'll get w2 if we pick a different request class:

    >>> pool.get('bar')
    w2

 We're gonna be white box and look at the pool data structures from
 time to time.

    >>> pool
    Request classes:
      bar: w2(1.0,1)
      foo: w1(1.0,2)
    Backlogs:
      1: [w2]
      2: [w1]

Here, we can see that w1 is used for the foo class and w2 for the bar
class.  In the request classes, the worker's score and it's overall
backlog if shown in paretheses.  We see that both workers have a score
of 1.0.  This is the default score for new workers.  We'll say more
about this later.

Let's add another worker:

    >>> w3 = Worker('w3')
    >>> pool.new_resume(w3, {})

and make some more foo requests:

    >>> [pool.get('foo') for i in range(3)]
    [w1, w1, w1]

    >>> pool
    Request classes:
      bar: w2(1.0,1)
      foo: w1(1.0,5)
    Backlogs:
      0: [w3]
      1: [w2]
      5: [w1]

Even though we still had a worker with no backlog, we kept sending
requests to w1.  This is because w1 hasn't reached it's maximum
backlog.  Also, it's score is greater than the min score, which
defaults to 1.0.  Let's reduce the maximum backlog to 5:

    >>> pool.max_backlog = 5

So now, w1 has reached it's maximum backlog.  If
we make another foo request, we'll start using w3, and when that's
reached it's maximum backlog, we'll start using w2:

    >>> [pool.get('foo') for i in range(7)]
    [w3, w3, w3, w3, w3, w2, w2]

    >>> pool
    Request classes:
      bar: w2(1.0,3)
      foo: w1(1.0,5), w2(1.0,3), w3(1.0,5)
    Backlogs:
      3: [w2]
      5: [w1, w3]

If we get all workers to the maximum backlog, we'll block until a
worker is free.

    >>> [pool.get('foo') for i in range(2)]
    [w2, w2]

    >>> pool.get('foo', 0.0)

When a worker is done doing it's work, we put it back in the pool:

    >>> pool.put(w1)
    >>> pool.put(w1)
    >>> pool.put(w1)
    >>> pool.put(w2)
    >>> pool.put(w3)
    >>> pool.put(w3)
    >>> pool
    Request classes:
      bar: w2(1.0,4)
      foo: w1(1.0,2), w2(1.0,4), w3(1.0,3)
    Backlogs:
      2: [w1]
      3: [w3]
      4: [w2]

Now, when we get a worker, we'll get w1.

    >>> pool.get('foo', 0.0)
    w1

Why? We adjust each score by the worker's backlog, so even though all
2 workers had the same score, w1 is chosen because it has the smallest
backlog.

Now that we've done some work, let's update the resumes.  This will
normally be done by workers periodically, after collecting performance
data.

    >>> pool.new_resume(w1, {'foo': 6.0})

    >>> pool.new_resume(w2, {'bar': 2.0, 'foo': 2.0})

    >>> pool.new_resume(w3, {'foo': 3.8})

    >>> pool
    Request classes:
      bar: w2(2.0,4)
      foo: w2(2.0,4), w3(3.8,3), w1(6.0,3)
    Backlogs:
      3: [w1, w3]
      4: [w2]

    >>> pool.get('foo')
    w1
    >>> pool.get('foo')
    w1

    >>> pool
    Request classes:
      bar: w2(2.0,4)
      foo: w2(2.0,4), w3(3.8,3), w1(6.0,5)
    Backlogs:
      3: [w3]
      4: [w2]
      5: [w1]

Because w1 has reached the maximum backlog, it's out of the running.

    >>> pool.get('foo')
    w3
    >>> pool.get('foo')
    w3
    >>> pool.get('foo')
    w2

    >>> pool.put(w1)
    >>> pool.put(w3)
    >>> pool.put(w3)
    >>> pool.put(w3)
    >>> pool.put(w3)
    >>> pool.put(w3)
    >>> pool
    Request classes:
      bar: w2(2.0,5)
      foo: w2(2.0,5), w3(3.8,0), w1(6.0,4)
    Backlogs:
      0: [w3]
      4: [w1]
      5: [w2]

    >>> [pool.get('foo') for i in range(5)]
    [w3, w3, w3, w1, w3]

Pool settings
=============

There are several settings that effect pools:

max_backlog
  Maximum worker backlog, defaulting to 40.

min_score
  A worker won't be used if it has a backlog greater than 1 and it's
  score is less than min_score.

unskilled_score
  The score assigned to workers when given a new request class.  This
  defaults to min_score.

We've already seen max_backlog at work.  Let's test the other
settings.

Worker disconnect
=================

When a worker disconnect, it's removed from the pool:

    >>> pool.remove(w1)
    >>> pool.remove(w3)
    >>> pool
    Request classes:
      bar: w2(2.0,5)
      foo: w2(2.0,5)
    Backlogs:
      5: [w2]
